{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Setting up Environment </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "nb_actions = 5\n",
    "i_width = 227\n",
    "i_height = 227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.core import Env\n",
    "import numpy as np\n",
    "import sys\n",
    "import scipy.misc\n",
    "sys.path.append('/home/arusia/fastai/airsim/AirSim/PythonClient')\n",
    "from AirSimClient import *\n",
    "class simulation(Env):\n",
    "    import time\n",
    "    reward_range = (-np.inf, np.inf)\n",
    "    observation_space = None\n",
    "    solar_coordinates = [3800,180,100]\n",
    "    old_dist=0\n",
    "    scale_reward_1=0.3\n",
    "    state=None\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # connect to the AirSim simulator\n",
    "        self.client = MultirotorClient()\n",
    "        self.client.confirmConnection()\n",
    "        self.client.enableApiControl(True)\n",
    "        self.client.armDisarm(True)\n",
    "        self.client.takeoff()\n",
    "        self.state=self.client.getMultirotorState()\n",
    "        \n",
    "        \n",
    "    def get_image(self):\n",
    "        image_response = self.client.simGetImages([ImageRequest(0, AirSimImageType.Scene, False, False)])[0]\n",
    "        image1d = np.fromstring(image_response.image_data_uint8, dtype=np.uint8)\n",
    "        image_rgba = image1d.reshape(image_response.height, image_response.width, 4)\n",
    "        image_rgba = image_rgba[:,:,:3]\n",
    "        img=scipy.misc.imresize(image_rgba,[i_height, i_width])\n",
    "        return img\n",
    "    \n",
    "    def get_distance(self):\n",
    "        alt=self.state.gps_location.altitude\n",
    "        lat=self.state.gps_location.latitude\n",
    "        lon=self.state.gps_location.longitude\n",
    "        coord=np.array([lat,lon,alt])\n",
    "        return np.sqrt(np.linalg.norm(coord-self.solar_coordinates))\n",
    "    \n",
    "    def get_reward(self):\n",
    "        self.new_dist=self.get_distance()\n",
    "        if(self.state.collision.has_collided==True):\n",
    "            reward=-1\n",
    "            return\n",
    "        if(self.new_dist<10):\n",
    "            reward=1\n",
    "            return\n",
    "        reward = self.scale_reward_1/(-self.new_dist+self.old_dist)\n",
    "        self.old_dist=self.new_dist\n",
    "        return reward\n",
    "        \n",
    "    def step(self, action):\n",
    "        #perform action\n",
    "        #find the direction to move to\n",
    "        normpara=np.linalg.norm(action[0:3])\n",
    "        direc = action[0:3]/normpara\n",
    "        \n",
    "        #find the velocity in each direction\n",
    "        velocity = direc * action[3]\n",
    "        \n",
    "        self.client.moveByVelocity(velocity[0], velocity[1], velocity[2], action[4])\n",
    "        time.sleep(action[4])\n",
    "        \n",
    "        #Get observations\n",
    "        self.state=self.client.getMultirotorState()\n",
    "        observation=self.get_image()\n",
    "        reward = self.get_reward()\n",
    "        done=False\n",
    "        if(self.new_dist<10):\n",
    "            done=True\n",
    "        info=None\n",
    "        return observation,reward,done,info\n",
    "    \n",
    "    def reset(self):\n",
    "        self.client.reset()\n",
    "        client = MultirotorClient()\n",
    "        client.confirmConnection()\n",
    "        self.client.enableApiControl(True)\n",
    "        self.client.armDisarm(True)\n",
    "        client.takeoff()\n",
    "        self.old_dist=self.get_distance()\n",
    "        observation=self.get_image()\n",
    "        self.state=self.client.getMultirotorState()\n",
    "        return observation\n",
    "    \n",
    "    def delete(self):\n",
    "        self.client.reset()\n",
    "        self.client.enableApiControl(False)\n",
    "\n",
    "        \n",
    "    def a_space(self):\n",
    "        def sample(self,seed=None):\n",
    "            direction=-1+np.random.random(1)*2\n",
    "            speed_time=np.random.random(1)\n",
    "            return [direction[0],direction[1],direction[2],speed_time[0],speed_time[1]]\n",
    "        def contains(self,x):\n",
    "            if(max(x[0:3])>1 or min(x[0:3])<-1):\n",
    "                return False\n",
    "            \n",
    "            if(max(x[0:3])>1 or min(x[0:3])<0):\n",
    "                return False\n",
    "            return True\n",
    "        \n",
    "    action_space = a_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Getting vanilla SqueezeNet to be used as the driver </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras_squeezenet import SqueezeNet\n",
    "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.layers import Dense,Flatten,Input,Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "def get_actor():\n",
    "    model = SqueezeNet()\n",
    "    hid=Dense(512,activation='relu')(model.layers[-2].output)\n",
    "    x=Dense(1, activation='tanh')(hid)\n",
    "    y=Dense(1, activation='tanh')(hid)\n",
    "    z=Dense(1, activation='tanh')(hid)\n",
    "    v=Dense(1, activation='sigmoid')(hid)\n",
    "    t=Dense(1, activation='sigmoid')(hid)\n",
    "    out=Concatenate()([x,y,z,v,t])\n",
    "    Driver = Model(inputs=model.layers[0].output, outputs=out)\n",
    "    return Driver\n",
    "\n",
    "def get_critic():\n",
    "    model = SqueezeNet()\n",
    "    \n",
    "    features=model.layers[-2].output\n",
    "    action=Input(shape=(nb_actions,),name=\"input_action\")\n",
    "    conc=Concatenate()([features,action])\n",
    "    hid=Dense(512,activation='relu')(conc)\n",
    "    output_layer=Dense(1, activation='sigmoid')(hid)\n",
    "    Driver = Model(inputs=[model.layers[0].output,action], outputs=output_layer)\n",
    "    return Driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Writing a reinforcement learning routine</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "class random_proc(OrnsteinUhlenbeckProcess):\n",
    "    def sample(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n",
    "        x[self.size-1]=abs(x[self.size-1])\n",
    "        self.x_prev = x\n",
    "        self.n_steps += 1\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for connection: \n",
      "Training for 50000 steps ...\n",
      "Waiting for connection: \n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arusia/miniconda2/lib/python2.7/site-packages/ipykernel_launcher.py:29: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "/home/arusia/miniconda2/lib/python2.7/site-packages/ipykernel_launcher.py:32: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected input_1 to have 4 dimensions, but got array with shape (1, 1, 227, 227, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7305204f8e2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# slows down training quite a lot. You can always safely abort the training prematurely using\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Ctrl + C.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_max_episode_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# After training is done, we save the final weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arusia/fastai/airsim/rl/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;31m# This is were all of the work happens. We first perceive and compute the action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;31m# (forward step) and then use the reward to improve (backward step).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arusia/fastai/airsim/rl/agents/ddpg.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# Select an action.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_recent_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# TODO: move this into policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arusia/fastai/airsim/rl/agents/ddpg.py\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_state_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arusia/miniconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1904\u001b[0m         \"\"\"\n\u001b[1;32m   1905\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[0;32m-> 1906\u001b[0;31m                                     self._feed_input_shapes)\n\u001b[0m\u001b[1;32m   1907\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arusia/miniconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    108\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected input_1 to have 4 dimensions, but got array with shape (1, 1, 227, 227, 3)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Input, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DDPGAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "\n",
    "\n",
    "ENV_NAME='airsim'\n",
    "env=simulation()\n",
    "# Get actor and critic\n",
    "actor=get_actor()\n",
    "critic=get_critic()\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "\n",
    "random_process = random_proc(size=nb_actions, theta=.15, mu=0., sigma=.3)\n",
    "\n",
    "agent = DDPGAgent(nb_actions=nb_actions, actor=actor, critic=critic, critic_action_input=critic.input[1],\n",
    "                  memory=memory, nb_steps_warmup_critic=100, nb_steps_warmup_actor=100,\n",
    "                  random_process=random_process, gamma=.97, target_model_update=1e-3)\n",
    "agent.compile(Adam(lr=.001, clipnorm=1.), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "agent.fit(env, nb_steps=50000, visualize=False, verbose=1, nb_max_episode_steps=200)\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "agent.save_weights('ddpg_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "agent.test(env, nb_episodes=5, visualize=True, nb_max_episode_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
